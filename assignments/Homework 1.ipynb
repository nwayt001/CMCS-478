{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2-pbHSHg_2xB"
      },
      "id": "2-pbHSHg_2xB",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "56362576",
      "metadata": {
        "id": "56362576"
      },
      "source": [
        "# Homework 1\n",
        "\n",
        "In this homework you will implement a numerical approach to gradient descent and use it to implement the perceptron algorithm. That will take place in stages described below. But we'll start by describing how to perform gradient descent numerically.\n",
        "The method of finite differences\n",
        "\n",
        "Given a function $f(x)$\n",
        ", the analytical approach to gradient descent - i.e., to finding the value of x that minimizes $f(x)$ - is to compute $f′(x)$ and iterate as follows, where $α∈(0,1]$ is the learning rate:\n",
        "\n",
        "    x = 0\n",
        "    while not converged:\n",
        "        x=x−αf′(x)\n",
        "\n",
        "If you cannot compute $f′(x)$\n",
        "analytically, you can estimate it as follows, for sufficiently small $ϵ$:\n",
        "\n",
        "$$f′(x)≈\\frac{f(x+ϵ)−f(x−ϵ)}{2ϵ}$$\n",
        "\n",
        "The method of finite differences makes the assumption that over very small intervals the function behaves linearly and the slope of the tangent line can be estimated as the difference between function values at the ends of the interval divided by the interval width.\n",
        "\n",
        "For example, the slope of $f(x)=x^2$\n",
        "at $x=1$ can be estimated as $f′(1)=\\frac{(1+0.001)^2−(1−0.001)^2}{2∗0.001}$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e = 0.001\n",
        "(math.pow(1 + e, 2) - math.pow(1 - e, 2))/(2*e)"
      ],
      "metadata": {
        "id": "Ndt-h_gD_S9R"
      },
      "id": "Ndt-h_gD_S9R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the number above is very close to what you would get analytically by taking derivatives: $f′(x)=2x$ so $f′(1)=2∗1=2$\n",
        "\n",
        "You'll use the method of finite differences to compute the derivative of a loss function with respect to the weights of a perceptron.\n",
        "\n",
        "Below is a simple implementation of the method of finite differences for a univariate function. It is overly simple, running for a fixed number of iterations, and assuming constants for $ϵ$\n",
        "and $α$, but it works for simple cases."
      ],
      "metadata": {
        "id": "K5iMvXsx_Tya"
      },
      "id": "K5iMvXsx_Tya"
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient decent using finite differences to compute the gradient to minimize x\n",
        "def fd_demo(f, x0):\n",
        "    e = 0.001\n",
        "    a = 0.01\n",
        "    x = x0\n",
        "    for _ in range(1000):\n",
        "        g = (f(x + e) - f(x - e)) / (2*e)\n",
        "        x = x - a * g\n",
        "    return x"
      ],
      "metadata": {
        "id": "aHdlHH-9_dWa"
      },
      "id": "aHdlHH-9_dWa",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x*x is minimized at 0\n",
        "f = lambda x: x * x\n",
        "fd_demo(f, 2)"
      ],
      "metadata": {
        "id": "tbYrTMxA_fmB",
        "outputId": "39475358-fb41-4cd5-dd65-43b650201af1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tbYrTMxA_fmB",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.365934714445534e-09"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sin(x) is minimized at lots of points, the closest to 2 is 3pi/2\n",
        "f = lambda x: math.sin(x)\n",
        "print(fd_demo(f, 2))\n",
        "print(math.pi * 3 / 2)\n"
      ],
      "metadata": {
        "id": "IiKhW3hI_u4y",
        "outputId": "484f8520-d365-417a-9278-bf737cc05a95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IiKhW3hI_u4y",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.711986618234112\n",
            "4.71238898038469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Perceptron class\n",
        "\n",
        "Below is a simple Perceptron class that you can use in this assignment. Feel free to use it as is, make changes, or throw it out and write your own. It creates a weight vector with small random numbers when initialized, and has methods to compute the activation for an input and to produce a binary class label for an input.\n",
        "\n",
        "Note that the class below does not maintain an explicit bias term b\n",
        ". You can add one or, better yet, make sure that all inputs, x, have a 1 in one of the positions."
      ],
      "metadata": {
        "id": "CjnswN8o_9pW"
      },
      "id": "CjnswN8o_9pW"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}